{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T08:40:47.318050Z",
     "start_time": "2024-12-19T08:40:47.313376Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import vgg19, VGG19_Weights\n",
    "from torchvision.transforms import transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T08:40:47.373956Z",
     "start_time": "2024-12-19T08:40:47.368161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "low_res_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "content_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "style_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Generator"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T08:40:47.425421Z",
     "start_time": "2024-12-19T08:40:47.420471Z"
    }
   },
   "source": [
    "class AdaINLayer(nn.Module):\n",
    "    def forward(self, content, style):\n",
    "        style_mean, style_std = style.mean([2, 3], keepdim=True), style.std([2, 3], keepdim=True)\n",
    "        content_mean, content_std = content.mean([2, 3], keepdim=True), content.std([2, 3], keepdim=True)\n",
    "        normalized_content = (content - content_mean) / content_std\n",
    "        return normalized_content * style_std + style_mean"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T08:40:47.474708Z",
     "start_time": "2024-12-19T08:40:47.469952Z"
    }
   },
   "source": [
    "class ResidualBlockAdaIN(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlockAdaIN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels,\n",
    "                               channels,\n",
    "                               kernel_size=3, \n",
    "                               stride=1, \n",
    "                               padding=1)\n",
    "        \n",
    "        self.adain1 = AdaINLayer()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(channels, \n",
    "                               channels, \n",
    "                               kernel_size=3, \n",
    "                               stride=1, \n",
    "                               padding=1)\n",
    "        \n",
    "        self.adain2 = AdaINLayer()\n",
    "\n",
    "    def forward(self, x, style_features):\n",
    "        # print(f'In Resnet class, self.conv1 shape is: {self.conv1(x).shape} while the style shape is: {style_features.shape}')\n",
    "        res = x\n",
    "        x = F.relu(self.adain1(self.conv1(x), style_features))\n",
    "        x = self.adain2(self.conv2(x), style_features)\n",
    "        return x + res"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class GeneratorB2A(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GeneratorB2A, self).__init__()\n",
    "\n",
    "        self.initial_conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 3, kernel_size=4, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.initial_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T08:40:47.537298Z",
     "start_time": "2024-12-19T08:40:47.527895Z"
    }
   },
   "source": [
    "class GeneratorA2B(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GeneratorA2B, self).__init__()\n",
    "        \n",
    "        self.initial_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, \n",
    "                      out_channels=64, \n",
    "                      kernel_size=7, \n",
    "                      stride=1, \n",
    "                      padding=3),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "        self.down1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, \n",
    "                      out_channels=128, \n",
    "                      kernel_size=3, \n",
    "                      stride=2, \n",
    "                      padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.down2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, \n",
    "                      out_channels=256, \n",
    "                      kernel_size=3, \n",
    "                      stride=2, \n",
    "                      padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            *[ResidualBlockAdaIN(256) for _ in range(6)]\n",
    "        )\n",
    "\n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=256, \n",
    "                               out_channels=128,\n",
    "                                kernel_size=3, \n",
    "                                stride=2, \n",
    "                                padding=1, \n",
    "                                output_padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.up2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=128, \n",
    "                               out_channels=64, \n",
    "                               kernel_size=3, \n",
    "                               stride=2, \n",
    "                               padding=1, \n",
    "                               output_padding=1),\n",
    "            nn.InstanceNorm2d(64), \n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.up3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=64,\n",
    "                               out_channels=32,\n",
    "                               kernel_size=3,\n",
    "                               stride=2,\n",
    "                               padding=1,\n",
    "                               output_padding=1),\n",
    "            nn.InstanceNorm2d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.up4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=32,\n",
    "                               out_channels=32,\n",
    "                               kernel_size=3,\n",
    "                               stride=2,\n",
    "                               padding=1,\n",
    "                               output_padding=1),\n",
    "            nn.InstanceNorm2d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "\n",
    "        self.final_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32,\n",
    "                      out_channels=3, \n",
    "                      kernel_size=7, \n",
    "                      stride=1, \n",
    "                      padding=3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, style_features):\n",
    "        print(f'This if from inside of the Generator, shape of x before passing through convs is: {x.shape}')\n",
    "        x = self.initial_conv(x)\n",
    "        x = self.down1(x)\n",
    "        x = self.down2(x)\n",
    "        for block in self.res_blocks:\n",
    "            x = block(x, style_features)\n",
    "            print(f'This if from inside of the Generator, shape of x after passing through resnet is: {x.shape}')\n",
    "        x = self.up1(x)\n",
    "        x = self.up2(x)\n",
    "        x = self.up3(x)\n",
    "        x = self.up4(x)\n",
    "        print(f'This if from inside of the Generator, shape of x after passing through upconvs is: {x.shape}')\n",
    "        \n",
    "        return self.final_conv(x)\n"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "+## Discriminator"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T08:40:47.575649Z",
     "start_time": "2024-12-19T08:40:47.571536Z"
    }
   },
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=4, stride=2, padding=0, padding_mode='reflect'):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels,\n",
    "                      out_channels,\n",
    "                      kernel_size,\n",
    "                      stride,\n",
    "                      padding,\n",
    "                      padding_mode='reflect'\n",
    "                      ),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T08:40:47.710976Z",
     "start_time": "2024-12-19T08:40:47.629221Z"
    }
   },
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3, features=[64, 128, 256, 512]):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.initial_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3,\n",
    "                      out_channels=64,\n",
    "                      kernel_size=4,\n",
    "                      stride=2,\n",
    "                      padding=1), \n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        layers = list()\n",
    "\n",
    "        in_channels = features[0]\n",
    "        for feature in features[1:]:\n",
    "            layers.append(\n",
    "                CNNBlock(\n",
    "                    in_channels,\n",
    "                    out_channels=feature,\n",
    "                    kernel_size=4,\n",
    "                    stride=1 if feature == features[-1] else 2,\n",
    "                    padding=0\n",
    "                )\n",
    "            )\n",
    "            in_channels = feature\n",
    "\n",
    "        layers.append(\n",
    "            nn.Conv2d(in_channels,\n",
    "                      out_channels=1,\n",
    "                      kernel_size=4,\n",
    "                      stride=1,\n",
    "                      padding=1,\n",
    "                      padding_mode='reflect')\n",
    "        )\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial_layer(x)\n",
    "        return torch.sigmoid(self.model(x))\n",
    "        \n",
    "\n",
    "def test():\n",
    "    x = torch.randn((5, 3, 256, 256))\n",
    "    model = Discriminator(in_channels=3)\n",
    "    preds = model(x)\n",
    "    print(preds.shape)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 26, 26])\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### WikiArt dataset"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T08:40:47.742984Z",
     "start_time": "2024-12-19T08:40:47.735705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CombinedDataset(Dataset):\n",
    "    def __init__(self, div2k_dir, wikiart_dir, low_res_transform, content_transform, style_transform):\n",
    "        self.div2k_dir = div2k_dir\n",
    "        self.wikiart_dir = wikiart_dir\n",
    "        self.div2k_filenames = [f for f in os.listdir(self.div2k_dir) if os.path.isfile(os.path.join(self.div2k_dir, f))]\n",
    "        self.wikiart_filenames = [f for f in os.listdir(wikiart_dir) if os.path.isfile(os.path.join(wikiart_dir, f))]\n",
    "\n",
    "        # self.wikiart = wikiart_dataset\n",
    "        self.low_res_transform = low_res_transform\n",
    "        self.content_transform = content_transform\n",
    "        self.style_transform = style_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the smaller of the two dataset lengths to avoid mismatch\n",
    "        return min(len(self.div2k_filenames), len(self.wikiart_filenames))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get low-resolution image from DIV2K\n",
    "        div2k_path = os.path.join(self.div2k_dir, self.div2k_filenames[idx])\n",
    "        wikiart_path = os.path.join(self.wikiart_dir, self.wikiart_filenames[idx % len(self.wikiart_filenames)])\n",
    "\n",
    "        div2k_img = Image.open(div2k_path).convert('RGB')\n",
    "        wikiart_img = Image.open(wikiart_path).convert('RGB')\n",
    "\n",
    "        # Get high-resolution stylized image from WikiArt\n",
    "        low_res_image = self.low_res_transform(div2k_img)\n",
    "        content_image = self.content_transform(div2k_img)  # Assuming key is \"image\"\n",
    "        style_image = self.style_transform(wikiart_img)  # Style image from WikiArt\n",
    "\n",
    "        return {\n",
    "            \"low_res\": low_res_image,\n",
    "            \"content\": content_image,\n",
    "            \"style\": style_image,\n",
    "        }\n"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T08:40:47.793789Z",
     "start_time": "2024-12-19T08:40:47.782575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wikiart_dataset_path = \"/home/mehran/Projects/SRStyle/Dataset/wikiart_shuffled/\"\n",
    "\n",
    "div2k_dataset_path = \"/home/mehran/Projects/SRStyle/Dataset/div2k_x2_train/\"\n",
    "combined_dataset = CombinedDataset(div2k_dataset_path, wikiart_dataset_path, low_res_transform, content_transform, style_transform)\n",
    "dataloader = DataLoader(combined_dataset, batch_size=4, shuffle=True, num_workers=4)"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T08:40:47.943834Z",
     "start_time": "2024-12-19T08:40:47.835361Z"
    }
   },
   "cell_type": "code",
   "source": "dataloader.dataset[0]",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'low_res': tensor([[[ 0.9294,  0.9294,  0.9294,  ...,  0.8745,  0.8824,  0.8824],\n",
       "          [ 0.9294,  0.9216,  0.9216,  ...,  0.8667,  0.8667,  0.8667],\n",
       "          [ 0.9059,  0.9373,  0.9294,  ...,  0.8431,  0.8353,  0.8275],\n",
       "          ...,\n",
       "          [-0.8431, -0.7020, -0.6392,  ..., -0.2314, -0.3412, -0.4431],\n",
       "          [-0.7647, -0.6706, -0.6549,  ..., -0.3882, -0.4824, -0.5373],\n",
       "          [-0.7255, -0.7255, -0.7098,  ..., -0.5059, -0.5608, -0.5529]],\n",
       " \n",
       "         [[ 0.9059,  0.9137,  0.9059,  ...,  0.8588,  0.8510,  0.8510],\n",
       "          [ 0.9059,  0.8980,  0.8980,  ...,  0.8431,  0.8353,  0.8353],\n",
       "          [ 0.8980,  0.9216,  0.9137,  ...,  0.8196,  0.8196,  0.8118],\n",
       "          ...,\n",
       "          [-0.7961, -0.7020, -0.7020,  ..., -0.2078, -0.3490, -0.4196],\n",
       "          [-0.7255, -0.6706, -0.6863,  ..., -0.4039, -0.4980, -0.5451],\n",
       "          [-0.6706, -0.7333, -0.6941,  ..., -0.5216, -0.5294, -0.5216]],\n",
       " \n",
       "         [[ 0.9373,  0.9373,  0.9451,  ...,  0.8745,  0.8745,  0.8745],\n",
       "          [ 0.9529,  0.9451,  0.9529,  ...,  0.8588,  0.8588,  0.8588],\n",
       "          [ 0.9529,  0.9765,  0.9765,  ...,  0.8431,  0.8353,  0.8275],\n",
       "          ...,\n",
       "          [-0.9608, -0.9294, -0.9137,  ..., -0.7098, -0.7490, -0.7569],\n",
       "          [-0.9216, -0.8824, -0.8824,  ..., -0.7333, -0.8039, -0.8118],\n",
       "          [-0.9137, -0.9216, -0.8980,  ..., -0.8039, -0.8431, -0.8275]]]),\n",
       " 'content': tensor([[[ 0.9294,  0.9373,  0.9373,  ...,  0.8902,  0.8902,  0.8902],\n",
       "          [ 0.9294,  0.9373,  0.9294,  ...,  0.8824,  0.8902,  0.8902],\n",
       "          [ 0.9294,  0.9294,  0.9294,  ...,  0.8745,  0.8824,  0.8824],\n",
       "          ...,\n",
       "          [-0.7020, -0.7098, -0.7333,  ..., -0.5059, -0.4980, -0.5843],\n",
       "          [-0.7333, -0.8118, -0.6941,  ..., -0.5451, -0.6235, -0.6000],\n",
       "          [-0.7569, -0.7412, -0.5529,  ..., -0.5294, -0.6235, -0.5765]],\n",
       " \n",
       "         [[ 0.9059,  0.9059,  0.9137,  ...,  0.8588,  0.8588,  0.8588],\n",
       "          [ 0.8980,  0.8980,  0.9059,  ...,  0.8510,  0.8510,  0.8510],\n",
       "          [ 0.8980,  0.9059,  0.8980,  ...,  0.8431,  0.8510,  0.8431],\n",
       "          ...,\n",
       "          [-0.6078, -0.6314, -0.6941,  ..., -0.4745, -0.5216, -0.5137],\n",
       "          [-0.6549, -0.7569, -0.6706,  ..., -0.5294, -0.5608, -0.5059],\n",
       "          [-0.6863, -0.6784, -0.5137,  ..., -0.5765, -0.6000, -0.4588]],\n",
       " \n",
       "         [[ 0.9294,  0.9294,  0.9373,  ...,  0.8824,  0.8824,  0.8902],\n",
       "          [ 0.9294,  0.9294,  0.9373,  ...,  0.8824,  0.8824,  0.8902],\n",
       "          [ 0.9216,  0.9451,  0.9451,  ...,  0.8745,  0.8667,  0.8745],\n",
       "          ...,\n",
       "          [-0.8902, -0.9216, -0.9216,  ..., -0.7882, -0.8118, -0.9137],\n",
       "          [-0.8824, -0.9373, -0.9216,  ..., -0.8196, -0.8980, -0.9137],\n",
       "          [-0.8902, -0.9216, -0.8902,  ..., -0.7804, -0.8824, -0.8039]]]),\n",
       " 'style': tensor([[[-1.1589, -1.1418, -1.1418,  ..., -0.9705, -1.0219, -1.0048],\n",
       "          [-1.1418, -1.1247, -1.1418,  ..., -0.8678, -0.9020, -0.9020],\n",
       "          [-1.1418, -1.1247, -1.1418,  ..., -0.7479, -0.8164, -0.7993],\n",
       "          ...,\n",
       "          [-1.7925, -1.7754, -1.7754,  ..., -1.8439, -1.8439, -1.8439],\n",
       "          [-1.7925, -1.7754, -1.7925,  ..., -1.8439, -1.8439, -1.8439],\n",
       "          [-1.8097, -1.8097, -1.7925,  ..., -1.8782, -1.8610, -1.8610]],\n",
       " \n",
       "         [[-1.4055, -1.3880, -1.3880,  ..., -1.2479, -1.2829, -1.2479],\n",
       "          [-1.3880, -1.3704, -1.3704,  ..., -1.2129, -1.2304, -1.2304],\n",
       "          [-1.3529, -1.3529, -1.3704,  ..., -1.1078, -1.1604, -1.1604],\n",
       "          ...,\n",
       "          [-1.7731, -1.7556, -1.7556,  ..., -1.7906, -1.7906, -1.7906],\n",
       "          [-1.7731, -1.7556, -1.7731,  ..., -1.7906, -1.7906, -1.7906],\n",
       "          [-1.7906, -1.7906, -1.7731,  ..., -1.8256, -1.8081, -1.8081]],\n",
       " \n",
       "         [[-1.1944, -1.1770, -1.1770,  ..., -1.1421, -1.1770, -1.1421],\n",
       "          [-1.1770, -1.1596, -1.1596,  ..., -1.1247, -1.1421, -1.1421],\n",
       "          [-1.1247, -1.1247, -1.1421,  ..., -1.0376, -1.0898, -1.0898],\n",
       "          ...,\n",
       "          [-1.5953, -1.5779, -1.5779,  ..., -1.5430, -1.5430, -1.5430],\n",
       "          [-1.5953, -1.5779, -1.5953,  ..., -1.5430, -1.5430, -1.5430],\n",
       "          [-1.6127, -1.6127, -1.5953,  ..., -1.5779, -1.5604, -1.5604]]])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T08:40:47.969150Z",
     "start_time": "2024-12-19T08:40:47.963176Z"
    }
   },
   "source": [
    "# === VGG Feature Extractor ===\n",
    "class VGGFeatures(nn.Module):\n",
    "    def __init__(self, layers_to_extract):\n",
    "        super(VGGFeatures, self).__init__()\n",
    "        vgg = models.vgg19(weights=VGG19_Weights.IMAGENET1K_V1).features\n",
    "        layer_map = {\n",
    "            \"conv1_1\": 0,\n",
    "            \"conv2_1\": 5,\n",
    "            \"conv3_1\": 10,\n",
    "            \"conv4_1\": 19,\n",
    "            \"conv5_1\": 28\n",
    "        }\n",
    "\n",
    "        self.required_layers = sorted([layer_map[layer] for layer in layers_to_extract])\n",
    "        self.model = vgg[:self.required_layers[-1] + 1]\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = {}\n",
    "        for i, layer in enumerate(self.model):\n",
    "            x = layer(x)  # Pass input through each layer\n",
    "            if i in self.required_layers:\n",
    "                features[f\"layer_{i}\"] = x\n",
    "        return features"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T08:40:48.018093Z",
     "start_time": "2024-12-19T08:40:48.009991Z"
    }
   },
   "source": [
    "# === Loss Functions ===\n",
    "def content_loss(content_features, generated_features):\n",
    "    return nn.MSELoss()(generated_features, content_features)\n",
    "\n",
    "\n",
    "def adversarial_loss(prediction, target_is_real):\n",
    "    target = torch.ones_like(prediction) if target_is_real else torch.zeros_like(prediction)\n",
    "    return nn.BCEWithLogitsLoss()(prediction, target)\n",
    "\n",
    "\n",
    "def style_loss(style_features, generated_features):\n",
    "    loss = 0\n",
    "    for sf, gf in zip(style_features.values(), generated_features.values()):\n",
    "        if sf.size() != gf.size():\n",
    "            raise RuntimeError(\"Style and generated features have different shapes.\")\n",
    "\n",
    "        # Compute MSE between feature maps directly\n",
    "        loss += torch.nn.functional.mse_loss(gf, sf)\n",
    "    return loss"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T08:40:48.132613Z",
     "start_time": "2024-12-19T08:40:48.122433Z"
    }
   },
   "source": [
    "# === Training Loop ===\n",
    "def train_model(generator_a2b, generator_b2a, discriminator_a, discriminator_b, vgg_features, optimizer_g,\n",
    "                optimizer_d_a, optimizer_d_b, dataloader, device, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        epoch_progress = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch [{epoch + 1}/{epochs}]\")\n",
    "        for i, data in epoch_progress:\n",
    "            low_res_batch = data['low_res']\n",
    "            style_batch = data['style']\n",
    "            content_batch = data['content']\n",
    "            low_res = low_res_batch.to(device)\n",
    "            style = style_batch.to(device)\n",
    "            content = content_batch.to(device)\n",
    "\n",
    "\n",
    "            # Extract VGG features for content and style\n",
    "            content_features = vgg_features(content)\n",
    "            style_features = vgg_features(style)\n",
    "\n",
    "            content_feature = content_features[\"layer_10\"]\n",
    "            style_feature = style_features[\"layer_10\"]\n",
    "\n",
    "            # === Generator A2B Training ===\n",
    "            optimizer_g.zero_grad()\n",
    "            generated_high_res = generator_a2b(low_res, style_feature)\n",
    "\n",
    "            generated_high_res_transform = transforms.Compose([\n",
    "            transforms.CenterCrop((224, 224)),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "            generated_features = vgg_features(generated_high_res_transform(generated_high_res))['layer_10']\n",
    "            c_loss = content_loss(content_feature, generated_features)\n",
    "            s_loss = style_loss(style_feature.to_sparse(), generated_features.to_sparse())\n",
    "            g_loss_adv = adversarial_loss(discriminator_b(generated_high_res), True)\n",
    "            loss_g_a2b = c_loss + s_loss + 0.1 * g_loss_adv  # Combine losses\n",
    "            loss_g_a2b.backward()\n",
    "            optimizer_g.step()\n",
    "\n",
    "            print('------------- Generator B2A Training... -----------------')\n",
    "            # === Generator B2A Training (Cycle Consistency) ===\n",
    "            optimizer_g.zero_grad()\n",
    "            reconstructed_low_res = generator_b2a(generated_high_res)\n",
    "            cycle_consistency_loss = content_loss(low_res, reconstructed_low_res)\n",
    "            g_loss_adv_b2a = adversarial_loss(discriminator_a(reconstructed_low_res), True)\n",
    "            loss_g_b2a = cycle_consistency_loss + 0.1 * g_loss_adv_b2a  # Combine losses\n",
    "\n",
    "            loss_g_b2a.backward()\n",
    "            optimizer_g.step()\n",
    "\n",
    "            print('$$$$$$$$$$$$$$$$$$ Discriminator A Training... $$$$$$$$$$$$$$$$$$$$')\n",
    "            # === Discriminator A Training ===\n",
    "            optimizer_d_a.zero_grad()\n",
    "            real_loss_a = adversarial_loss(discriminator_a(low_res), True)\n",
    "            fake_loss_a = adversarial_loss(discriminator_a(reconstructed_low_res.detach()), False)\n",
    "            loss_d_a = 0.5 * (real_loss_a + fake_loss_a)\n",
    "\n",
    "            loss_d_a.backward()\n",
    "            optimizer_d_b.step()\n",
    "\n",
    "            print('##################### Discriminator B Training... ######################')\n",
    "            # === Discriminator B Training ===\n",
    "            optimizer_d_b.zero_grad()\n",
    "            real_loss_b = adversarial_loss(discriminator_b(content), True)\n",
    "            fake_loss_b = adversarial_loss(discriminator_b(generated_high_res.detach()), False)\n",
    "            loss_d_b = 0.5 * (real_loss_b + fake_loss_b)\n",
    "\n",
    "            loss_d_b.backward()\n",
    "            optimizer_d_b.step()\n",
    "\n",
    "            # Update loop description\n",
    "            epoch_progress.set_postfix({\n",
    "                \"Loss_G_A2B\": loss_g_a2b.item(),\n",
    "                \"Loss_G_B2A\": loss_g_b2a.item(),\n",
    "                \"Loss_D_A\": loss_d_a.item(),\n",
    "                \"Loss_D_B\": loss_d_b.item()\n",
    "            })\n",
    "        print('()()()()()()()() Loop Description completed successfully!')\n",
    "        torch.save(generator_a2b.state_dict(), \"generator_a2b.pth\")\n",
    "        torch.save(generator_b2a.state_dict(), \"generator_b2a.pth\")\n",
    "        torch.save(discriminator_a.state_dict(), \"discriminator_a.pth\")\n",
    "        torch.save(discriminator_b.state_dict(), \"discriminator_b.pth\")\n"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "device = 'cuda'\n",
    "# Initialize the VGG feature extractor for content and style losses\n",
    "vgg_features = VGGFeatures(layers_to_extract=[\"conv1_1\", \"conv2_1\", \"conv3_1\", \"conv4_1\", \"conv5_1\"]).to(device)\n",
    "\n",
    "generator_a2b = GeneratorA2B()\n",
    "generator_b2a = GeneratorB2A()\n",
    "discriminator_a = Discriminator()\n",
    "discriminator_b = Discriminator()\n",
    "\n",
    "optimizer_g = torch.optim.Adam(\n",
    "    list(generator_a2b.parameters()) + list(generator_b2a.parameters()), lr=2e-4, betas=(0.5, 0.999)\n",
    ")\n",
    "optimizer_d_a = torch.optim.Adam(discriminator_a.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "optimizer_d_b = torch.optim.Adam(discriminator_b.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "\n",
    "# Move models to the device\n",
    "generator_a2b = generator_a2b.to(device)\n",
    "generator_b2a = generator_b2a.to(device)\n",
    "discriminator_a = discriminator_a.to(device)\n",
    "discriminator_b = discriminator_b.to(device)\n",
    "\n",
    "\n",
    "# Call the training loop\n",
    "train_model(\n",
    "    generator_a2b=generator_a2b,\n",
    "    generator_b2a=generator_b2a,\n",
    "    discriminator_a=discriminator_a,\n",
    "    discriminator_b=discriminator_b,\n",
    "    vgg_features=vgg_features,\n",
    "    optimizer_g=optimizer_g,\n",
    "    optimizer_d_a=optimizer_d_a,\n",
    "    optimizer_d_b=optimizer_d_b,\n",
    "    dataloader=dataloader,\n",
    "    device=device,\n",
    "    epochs=10  # Adjust the number of epochs\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
